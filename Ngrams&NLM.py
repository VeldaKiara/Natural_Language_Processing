""" n-gram model considers a sequence of some number (n) 
units and calculates the probability of each unit in a 
body of language given the preceding sequence of length n.
n-gram probabilities with larger n values can be impressive at 
language prediction.
A tactic known as language smoothing can help adjust 
probabilities for unknown words, but it isnâ€™t always ideal."""
